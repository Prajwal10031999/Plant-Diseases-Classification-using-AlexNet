{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nos.listdir(\"../input/modelalexnet-and-best-weights-9/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers.normalization import BatchNormalization\n\n# Initializing the CNN\nclassifier = Sequential()\n\n# Convolution Step 1\nclassifier.add(Convolution2D(96, 11, strides = (4, 4), padding = 'valid', input_shape=(224, 224, 3), activation = 'relu'))\n\n# Max Pooling Step 1\nclassifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\nclassifier.add(BatchNormalization())\n\n# Convolution Step 2\nclassifier.add(Convolution2D(256, 11, strides = (1, 1), padding='valid', activation = 'relu'))\n\n# Max Pooling Step 2\nclassifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding='valid'))\nclassifier.add(BatchNormalization())\n\n# Convolution Step 3\nclassifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\nclassifier.add(BatchNormalization())\n\n# Convolution Step 4\nclassifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\nclassifier.add(BatchNormalization())\n\n# Convolution Step 5\nclassifier.add(Convolution2D(256, 3, strides=(1,1), padding='valid', activation = 'relu'))\n\n# Max Pooling Step 3\nclassifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\nclassifier.add(BatchNormalization())\n\n# Flattening Step\nclassifier.add(Flatten())\n\n# Full Connection Step\nclassifier.add(Dense(units = 4096, activation = 'relu'))\nclassifier.add(Dropout(0.4))\nclassifier.add(BatchNormalization())\nclassifier.add(Dense(units = 4096, activation = 'relu'))\nclassifier.add(Dropout(0.4))\nclassifier.add(BatchNormalization())\nclassifier.add(Dense(units = 1000, activation = 'relu'))\nclassifier.add(Dropout(0.2))\nclassifier.add(BatchNormalization())\nclassifier.add(Dense(units = 38, activation = 'softmax'))\nclassifier.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.load_weights('../input/modelalexnet-and-best-weights-9/best_weights_9.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nfrom keras import layers\nfor i, layer in enumerate(classifier.layers):\n   print(i, layer.name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we chose to train the top 2 conv blocks, i.e. we will freeze\n# the first 8 layers and unfreeze the rest:\nprint(\"Freezed layers:\")\nfor i, layer in enumerate(classifier.layers[:20]):\n    print(i, layer.name)\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trainable parameters decrease after freezing some bottom layers   \nclassifier.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compiling the Model\nfrom keras import optimizers\nclassifier.compile(optimizer=optimizers.SGD(lr=0.001, momentum=0.9, decay=0.005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# image preprocessing\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   fill_mode='nearest')\n\nvalid_datagen = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 128\nbase_dir = \"../input/new-plant-diseases-dataset/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)\"\n\ntraining_set = train_datagen.flow_from_directory(base_dir+'/train',\n                                                 target_size=(224, 224),\n                                                 batch_size=batch_size,\n                                                 class_mode='categorical',\n                                                 shuffle=False)\n\nvalid_set = valid_datagen.flow_from_directory(base_dir+'/valid',\n                                            target_size=(224, 224),\n                                            batch_size=batch_size,\n                                            class_mode='categorical',\n                                            shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_dict = training_set.class_indices\nprint(class_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"li = list(class_dict.keys())\nprint(li)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"li = list(class_dict.keys())\nprint(li)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num = training_set.samples\nvalid_num = valid_set.samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\nfrom keras.callbacks import ModelCheckpoint\nweightpath = \"best_weights_9.hdf5\"\ncheckpoint = ModelCheckpoint(weightpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\n#fitting images to CNN\nhistory = classifier.fit_generator(training_set,\n                         steps_per_epoch=150, #train_num//batch_size,\n                         validation_data=valid_set,\n                         epochs=10, #25,\n                         validation_steps=100, #valid_num//batch_size,\n                         callbacks=callbacks_list)\n#saving model\nfilepath=\"AlexNetModel.hdf5\"\nclassifier.save(filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting training values\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='pink', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting an image\nfrom keras.preprocessing import image\nimport numpy as np\nimage_path = \"../input/new-plant-diseases-dataset/test/test/TomatoEarlyBlight1.JPG\"\nnew_img = image.load_img(image_path, target_size=(224, 224))\nimg = image.img_to_array(new_img)\nimg = np.expand_dims(img, axis=0)\nimg = img/255\n\nprint(\"Following is our prediction:\")\nprediction = classifier.predict(img)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nd = prediction.flatten()\nj = d.max()\nfor index,item in enumerate(d):\n    if item == j:\n        class_name = li[index]\n\n##Another way\n# img_class = classifier.predict_classes(img)\n# img_prob = classifier.predict_proba(img)\n# print(img_class ,img_prob )\n\n\n#ploting image with predicted class name        \nplt.figure(figsize = (4,4))\nplt.imshow(new_img)\nplt.axis('off')\nplt.title(class_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting an image\nfrom keras.preprocessing import image\nimport numpy as np\nimage_path = \"../input/new-plant-diseases-dataset/test/test/AppleCedarRust1.JPG\"\nnew_img = image.load_img(image_path, target_size=(224, 224))\nimg = image.img_to_array(new_img)\nimg = np.expand_dims(img, axis=0)\nimg = img/255\n\nprint(\"Following is our prediction:\")\nprediction = classifier.predict(img)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nd = prediction.flatten()\nj = d.max()\nfor index,item in enumerate(d):\n    if item == j:\n        class_name = li[index]\n\n##Another way\n# img_class = classifier.predict_classes(img)\n# img_prob = classifier.predict_proba(img)\n# print(img_class ,img_prob )\n\n\n#ploting image with predicted class name        \nplt.figure(figsize = (4,4))\nplt.imshow(new_img)\nplt.axis('off')\nplt.title(class_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reset the test_data to start iterating over dataset from scratch\nvalid_set.reset()\n# start to predict\npred = classifier.predict(valid_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\n# use the confusion_matrix function provided by tensorflow to generate confusion matrix\ncon_mat = tf.math.confusion_matrix(labels=valid_set.classes, predictions=np.argmax(pred, axis=1)).numpy()\n\n# normalize the confusion matrix\ncon_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n\n# convert the nomalized confusion matrix for better view\ncon_mat_df = pd.DataFrame(con_mat_norm,\n                     index = valid_set.class_indices.keys(), \n                     columns = valid_set.class_indices.keys())\n\n# show the nomalized confusion matrix\ncon_mat_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the original confusion matrix for better view (using the case numbers)\ncon_mat_df_explain = pd.DataFrame(con_mat,\n                     index = valid_set.class_indices.keys(), \n                     columns = valid_set.class_indices.keys())\n\n# show the unnomalized confusion matrix\ncon_mat_df_explain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics\n# generate the clasification report by using the classification_report of sklearn package\nreport = sklearn.metrics.classification_report(valid_set.classes, np.argmax(pred, axis=1), target_names=valid_set.class_indices.keys())\n\n# print the report\nprint(report)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}